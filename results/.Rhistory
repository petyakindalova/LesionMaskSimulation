n_covs_cont = 1 # number of continuous covariates
n_covs = n_covs_cat + n_covs_cont
empir_avail = 1 # flag: 0 (empirical probability is not available), 1 (empirical probability is available)
n_cores = 1 # number of cores for parallel GLM
link_fn = "probit" # options are logit and probit
method = 2 # 1: ML, 2: MeanBR
method_name = "MeanBR" # or change to "MeanBR"
brain_mask = readNIfTI(paste0(PATH_PROJ, "/data/MNI152_T1_2mm_brain_mask.nii.gz")) #MNI 2mm brain mask
load(paste0(PATH_TEMP, "/step2.RData"))
#our proposed option is to parallelize in subsets of voxels
subset_size = 1000
n_subsets = ceiling(nrow(lesions_subj)/subset_size)
for(i in 1:n_subsets){
load(paste0(PATH_TEMP, "/step2.RData"))
#determine indices
if(i == n_subsets) {
if(subset_size*(i-1)+1==nrow(lesions_subj)) {
subset_idx = nrow(lesions_subj)
} else {
subset_idx = seq(subset_size*(i-1)+1, nrow(lesions_subj), by = 1)
}
} else {
subset_idx = seq(subset_size*(i - 1)+1, subset_size*i , by = 1)
}
print(i)
lesions_subj_temp= lesions_subj[subset_idx,]
print(dim(lesions_subj_temp))
if(length(subset_idx)==1) {
lesions_subj_temp = as.matrix(as.vector(lesions_subj[subset_idx,]), nrow=1)
lesions_subj_temp = t(lesions_subj_temp)
print(dim(lesions_subj_temp))
}
rm(lesions_subj)
gc()
#run GLM
glm_results = fit_glm_fn(datafile = training_dataset,
lesionmat = lesions_subj_temp,
n_covs_cat = n_covs_cat,
n_covs_cont = n_covs_cont,
GLMmethod = method,
link_fn = link_fn,
outputdir = PATH_TEMP,
subset = i, n_cores = n_cores)
rm(glm_results)
gc()
print(i)
}
n_covs_cat = 0 # number of categorical covariates
n_covs_cont = 1 # number of continuous covariates
n_covs = n_covs_cat + n_covs_cont
empir_avail = 1 # flag: 0 (empirical probability is not available), 1 (empirical probability is available)
n_cores = 1 # number of cores for parallel GLM
link_fn = "probit" # options are logit and probit
method = 2 # 1: ML, 2: MeanBR
method_name = "MeanBR" # or change to "MeanBR"
brain_mask = readNIfTI(paste0(PATH_PROJ, "/data/MNI152_T1_2mm_brain_mask.nii.gz")) #MNI 2mm brain mask
n_covs_cat = 0 # number of categorical covariates
n_covs_cont = 1 # number of continuous covariates
n_covs = n_covs_cat + n_covs_cont
empir_avail = 1 # flag: 0 (empirical probability is not available), 1 (empirical probability is available)
n_cores = 1 # number of cores for parallel GLM
link_fn = "probit" # options are logit and probit
method = 2 # 1: ML, 2: MeanBR
method_name = "MeanBR" # or change to "MeanBR"
brain_mask = readNIfTI(paste0(PATH_PROJ, "/data/MNI152_T1_2mm_brain_mask.nii.gz")) #MNI 2mm brain mask
n_cores = 1 # number of cores for parallel GLM
#install.packages("rmarkdown")
library(oro.nifti)
library(pryr)
library(enrichwith)
library(lpSolveAPI)
library(brglm2)
library(detectseparation)
library(devtools)
library(waldi)
library(foreach)
library(doParallel)
library(iterators)
PATH_PROJ = "D:/Neuroimaging/Simulations/LesionMaskSimulation/" # Project path
#
PATH_DATA = file.path(PATH_PROJ, 'data', 'simulated_data') # Path where simulated lesion masks are stored
training_dataset = read.table(paste0(PATH_DATA, "/GLM_sample1000.dat"), header=T)
#
PATH_TEMP = file.path(PATH_PROJ, 'data', 'temp') # Path where temporary files will be stored
PATH_RESULTS = file.path(PATH_PROJ, 'results', 'MeanBR') # Path where GLM results are saved
#PATH_RESULTS = file.path(PATH_PROJ, 'results', 'ML') # change to ML if you are planning to obtain MLEs
#
PATH_SRC = file.path(PATH_PROJ, 'src') # Path where some help functions are stored
source(paste0(PATH_SRC, "/empir_prob_step1.R"))
source(paste0(PATH_SRC, "/lesion_matrix_step2.R"))
source(paste0(PATH_SRC, "/glm_step3.R"))
source(paste0(PATH_SRC, "/map_to_masks_step4.R"))
n_covs_cat = 0 # number of categorical covariates
n_covs_cont = 1 # number of continuous covariates
n_covs = n_covs_cat + n_covs_cont
empir_avail = 1 # flag: 0 (empirical probability is not available), 1 (empirical probability is available)
n_cores = 1 # number of cores for parallel GLM
link_fn = "probit" # options are logit and probit
method = 2 # 1: ML, 2: MeanBR
method_name = "MeanBR" # or change to "MeanBR"
brain_mask = readNIfTI(paste0(PATH_PROJ, "/data/MNI152_T1_2mm_brain_mask.nii.gz")) #MNI 2mm brain mask
load(paste0(PATH_TEMP, "/step2.RData"))
#our proposed option is to parallelize in subsets of voxels
subset_size = 1000
n_subsets = ceiling(nrow(lesions_subj)/subset_size)
for(i in 1:n_subsets){
load(paste0(PATH_TEMP, "/step2.RData"))
#determine indices
if(i == n_subsets) {
if(subset_size*(i-1)+1==nrow(lesions_subj)) {
subset_idx = nrow(lesions_subj)
} else {
subset_idx = seq(subset_size*(i-1)+1, nrow(lesions_subj), by = 1)
}
} else {
subset_idx = seq(subset_size*(i - 1)+1, subset_size*i , by = 1)
}
print(i)
lesions_subj_temp= lesions_subj[subset_idx,]
print(dim(lesions_subj_temp))
if(length(subset_idx)==1) {
lesions_subj_temp = as.matrix(as.vector(lesions_subj[subset_idx,]), nrow=1)
lesions_subj_temp = t(lesions_subj_temp)
print(dim(lesions_subj_temp))
}
rm(lesions_subj)
gc()
#run GLM
glm_results = fit_glm_fn(datafile = training_dataset,
lesionmat = lesions_subj_temp,
n_covs_cat = n_covs_cat,
n_covs_cont = n_covs_cont,
GLMmethod = method,
link_fn = link_fn,
outputdir = PATH_TEMP,
subset = i, n_cores = n_cores)
rm(glm_results)
gc()
print(i)
}
load(paste0(PATH_TEMP, "/step2.RData"))
#our proposed option is to parallelize in subsets of voxels
subset_size = 1000
n_subsets = ceiling(nrow(lesions_subj)/subset_size)
for(i in 1:n_subsets){
load(paste0(PATH_TEMP, "/step2.RData"))
#determine indices
if(i == n_subsets) {
if(subset_size*(i-1)+1==nrow(lesions_subj)) {
subset_idx = nrow(lesions_subj)
} else {
subset_idx = seq(subset_size*(i-1)+1, nrow(lesions_subj), by = 1)
}
} else {
subset_idx = seq(subset_size*(i - 1)+1, subset_size*i , by = 1)
}
print(i)
lesions_subj_temp= lesions_subj[subset_idx,]
print(dim(lesions_subj_temp))
if(length(subset_idx)==1) {
lesions_subj_temp = as.matrix(as.vector(lesions_subj[subset_idx,]), nrow=1)
lesions_subj_temp = t(lesions_subj_temp)
print(dim(lesions_subj_temp))
}
rm(lesions_subj)
gc()
n_cores = 1
#run GLM
glm_results = fit_glm_fn(datafile = training_dataset,
lesionmat = lesions_subj_temp,
n_covs_cat = n_covs_cat,
n_covs_cont = n_covs_cont,
GLMmethod = method,
link_fn = link_fn,
outputdir = PATH_TEMP,
subset = i, n_cores = n_cores)
rm(glm_results)
gc()
print(i)
}
i
j = 1
current_data = data.frame(voxel_lesion = as.factor(as.vector(lesionmat[j,1:n_subjects])), datafile)
library(doParallel)
library(iterators)
load(paste0(PATH_TEMP, "/step2.RData"))
library(oro.nifti)
library(pryr)
library(enrichwith)
library(lpSolveAPI)
library(brglm2)
library(detectseparation)
library(devtools)
library(waldi)
library(foreach)
library(doParallel)
library(iterators)
PATH_PROJ = "D:/Neuroimaging/Simulations/LesionMaskSimulation/" # Project path
#
PATH_DATA = file.path(PATH_PROJ, 'data', 'simulated_data') # Path where simulated lesion masks are stored
training_dataset = read.table(paste0(PATH_DATA, "/GLM_sample1000.dat"), header=T)
#
PATH_TEMP = file.path(PATH_PROJ, 'data', 'temp') # Path where temporary files will be stored
PATH_RESULTS = file.path(PATH_PROJ, 'results', 'MeanBR') # Path where GLM results are saved
#PATH_RESULTS = file.path(PATH_PROJ, 'results', 'ML') # change to ML if you are planning to obtain MLEs
#
PATH_SRC = file.path(PATH_PROJ, 'src') # Path where some help functions are stored
source(paste0(PATH_SRC, "/empir_prob_step1.R"))
source(paste0(PATH_SRC, "/lesion_matrix_step2.R"))
source(paste0(PATH_SRC, "/glm_step3.R"))
source(paste0(PATH_SRC, "/map_to_masks_step4.R"))
n_covs_cat = 0 # number of categorical covariates
n_covs_cont = 1 # number of continuous covariates
n_covs = n_covs_cat + n_covs_cont
empir_avail = 1 # flag: 0 (empirical probability is not available), 1 (empirical probability is available)
n_cores = 1 # number of cores for parallel GLM
link_fn = "probit" # options are logit and probit
method = 2 # 1: ML, 2: MeanBR
method_name = "MeanBR" # or change to "MeanBR"
brain_mask = readNIfTI(paste0(PATH_PROJ, "/data/MNI152_T1_2mm_brain_mask.nii.gz")) #MNI 2mm brain mask
load(paste0(PATH_TEMP, "/step2.RData"))
#our proposed option is to parallelize in subsets of voxels
subset_size = 1000
n_subsets = ceiling(nrow(lesions_subj)/subset_size)
load(paste0(PATH_TEMP, "/step2.RData"))
n_subsets
#determine indices
if(i == n_subsets) {
if(subset_size*(i-1)+1==nrow(lesions_subj)) {
subset_idx = nrow(lesions_subj)
} else {
subset_idx = seq(subset_size*(i-1)+1, nrow(lesions_subj), by = 1)
}
} else {
subset_idx = seq(subset_size*(i - 1)+1, subset_size*i , by = 1)
}
i =1
#determine indices
if(i == n_subsets) {
if(subset_size*(i-1)+1==nrow(lesions_subj)) {
subset_idx = nrow(lesions_subj)
} else {
subset_idx = seq(subset_size*(i-1)+1, nrow(lesions_subj), by = 1)
}
} else {
subset_idx = seq(subset_size*(i - 1)+1, subset_size*i , by = 1)
}
subset_idx
lesions_subj_temp= lesions_subj[subset_idx,]
print(dim(lesions_subj_temp))
is.na(lesions_subj_temp)
sum(is.na(lesions_subj_temp))
current_data = data.frame(voxel_lesion = as.factor(as.vector(lesionmat[j,1:n_subjects])), datafile)
datafile = training_dataset
lesionmat = lesions_subj_temp
n_covs_cat = n_covs_cat
GLMmethod = method
link_fn = link_fn
outputdir = PATH_TEMP
n_cores = n_cores
current_data = data.frame(voxel_lesion = as.factor(as.vector(lesionmat[j,1:n_subjects])), datafile)
n_subjects = nrow(datafile)
#we have subject id and file name as first and last column of dataframe, i.e. (-2) below
n_covs = ncol(datafile) - 2
#check number of covariates
if(n_covs != (n_covs_cat + n_covs_cont)) stop("Something is wrong with data table or number of covariate supplied!")
## Determine the model formula, to be used for each voxel-specific fit
names_data = names(datafile)
if (n_covs_cat > 0) {covs_cat_ind = seq(2, (n_covs_cat+1), by = 1)}
if(n_covs_cont > 0) {covs_cont_ind = (n_covs_cat + 2):(ncol(datafile)-1)}
factor_vars = ""
cont_vars = ""
if(n_covs_cat > 0) {
for (i in 1: length(covs_cat_ind)) {
if (i == 1) {
factor_vars = paste0("factor(",names_data[covs_cat_ind[i]],")")
} else {
factor_vars = paste0(factor_vars, " + factor(", names_data[covs_cat_ind[i]], ")")
}
}
}
factor_vars
if(n_covs_cont > 0) {
for (i in 1:length(covs_cont_ind)) {
if (i == 1) {
cont_vars = names_data[covs_cont_ind[i]]
} else {
cont_vars = paste0(cont_vars, " + ", names_data[covs_cont_ind[i]])
}
#make sure the continuous covariates are demeaned
datafile[,names_data[covs_cont_ind[i]]] = scale(datafile[,names_data[covs_cont_ind[i]]], scale=F)
}
}
cont_vars
if (factor_vars == "") {
all_vars = paste0(cont_vars)
} else if(cont_vars==""){
all_vars = paste0(factor_vars)
} else {
all_vars = paste0(factor_vars," + ", cont_vars)
}
base_formula <- paste("voxel_lesion ~", all_vars)
print(base_formula)
current_data = data.frame(voxel_lesion = as.factor(as.vector(lesionmat[j,1:n_subjects])), datafile)
head(current_data)
fit_br = glm(base_formula, family = binomial(link_fn), data = current_data,
method = "brglmFit", type = "AS_mean", maxit = 10000, epsilon = 1e-05, slowit=0.5)
base_formula
is.na(current_data)
j
j = 2
current_data = data.frame(voxel_lesion = as.factor(as.vector(lesionmat[j,1:n_subjects])), datafile)
fit_br = glm(base_formula, family = binomial(link_fn), data = current_data,
method = "brglmFit", type = "AS_mean", maxit = 10000, epsilon = 1e-05, slowit=0.5)
#install.packages("rmarkdown")
knitr::opts_chunk$set(echo = TRUE)
#install.packages("oro.nifti")
#install.packages("pryr")
#install.packages("enrichwith")
#install.packages("lpSolveAPI")
#install.packages("brglm2")
#install.packages("detectseparation")
#install.packages("devtools")
#devtools::install_github("ikosmidis/waldi", force=T)
#install.packages("foreach")
#install.packages("doParallel")
#install.packages("iterators")
library(oro.nifti)
library(pryr)
library(enrichwith)
library(lpSolveAPI)
library(brglm2)
library(detectseparation)
library(devtools)
library(waldi)
library(foreach)
library(doParallel)
library(iterators)
PATH_PROJ = "D:/Neuroimaging/Simulations/LesionMaskSimulation/" # Project path
#
PATH_DATA = file.path(PATH_PROJ, 'data', 'simulated_data') # Path where simulated lesion masks are stored
training_dataset = read.table(paste0(PATH_DATA, "/GLM_sample1000.dat"), header=T)
#
PATH_TEMP = file.path(PATH_PROJ, 'data', 'temp') # Path where temporary files will be stored
PATH_RESULTS = file.path(PATH_PROJ, 'results', 'MeanBR') # Path where GLM results are saved
#PATH_RESULTS = file.path(PATH_PROJ, 'results', 'ML') # change to ML if you are planning to obtain MLEs
#
PATH_SRC = file.path(PATH_PROJ, 'src') # Path where some help functions are stored
source(paste0(PATH_SRC, "/empir_prob_step1.R"))
source(paste0(PATH_SRC, "/lesion_matrix_step2.R"))
source(paste0(PATH_SRC, "/glm_step3.R"))
source(paste0(PATH_SRC, "/map_to_masks_step4.R"))
n_covs_cat = 0 # number of categorical covariates
n_covs_cont = 1 # number of continuous covariates
n_covs = n_covs_cat + n_covs_cont
empir_avail = 1 # flag: 0 (empirical probability is not available), 1 (empirical probability is available)
n_cores = 1 # number of cores for parallel GLM
link_fn = "probit" # options are logit and probit
method = 2 # 1: ML, 2: MeanBR
method_name = "MeanBR" # or change to "MeanBR"
brain_mask = readNIfTI(paste0(PATH_PROJ, "/data/MNI152_T1_2mm_brain_mask.nii.gz")) #MNI 2mm brain mask
mapping = read.table(paste0(PATH_TEMP, "/voxel_IDs.dat"), header=F)
mapping = as.matrix(mapping, nrow = 1)
n_subsets = ceiling(length(mapping)/1000)
print("number of subsets of voxels")
print(n_subsets)
filename_subset = paste0(PATH_TEMP, "/GLM_subset_",1, "_", method_name, "_Nvars_", n_covs,"_results.RData")
load(filename_subset)
n_coefs = length(output[[1]]$parameter) #obtain number of coefficients since it may differ from n_covs
Sys.time()
mapping_fn_vol2(brain_mask = brain_mask,
tempdir = PATH_TEMP,
mapping = mapping,
n_subsets = n_subsets,
n_covs = n_covs,
n_coefs = n_coefs,
GLMmethod = method,
outputdir = PATH_RESULTS)
mapping = read.table(paste0(PATH_TEMP, "/voxel_IDs.dat"), header=F)
mapping = as.matrix(mapping, nrow = 1)
n_subsets = ceiling(length(mapping)/1000)
print("number of subsets of voxels")
print(n_subsets)
filename_subset = paste0(PATH_TEMP, "/GLM_subset_",1, "_", method_name, "_Nvars_", n_covs,"_results.RData")
load(filename_subset)
n_coefs = length(output[[1]]$parameter) #obtain number of coefficients since it may differ from n_covs
Sys.time()
mapping_fn_vol2(brain_mask = brain_mask,
tempdir = PATH_TEMP,
mapping = mapping,
n_subsets = n_subsets,
n_covs = n_covs,
n_coefs = n_coefs,
GLMmethod = method,
outputdir = PATH_RESULTS)
Sys.time()
PATH_PROJ = "D:/Neuroimaging/Simulations/LesionMaskSimulation/" # Project path
#
PATH_DATA = file.path(PATH_PROJ, 'data', 'simulated_data') # Path where simulated lesion masks are stored
training_dataset = read.table(paste0(PATH_DATA, "/GLM_sample1000.dat"), header=T)
#
PATH_TEMP = file.path(PATH_PROJ, 'data', 'temp') # Path where temporary files will be stored
PATH_RESULTS = file.path(PATH_PROJ, 'results', 'MeanBR') # Path where GLM results are saved
#PATH_RESULTS = file.path(PATH_PROJ, 'results', 'ML') # change to ML if you are planning to obtain MLEs
#
PATH_SRC = file.path(PATH_PROJ, 'src') # Path where some help functions are stored
source(paste0(PATH_SRC, "/empir_prob_step1.R"))
source(paste0(PATH_SRC, "/lesion_matrix_step2.R"))
source(paste0(PATH_SRC, "/glm_step3.R"))
source(paste0(PATH_SRC, "/map_to_masks_step4.R"))
if(!empir_avail) {
start_time = Sys.time()
temp = empir_prob(datafile = training_dataset,
imagedir = PATH_DATA,
outputdir = PATH_TEMP,
voxel_IDs = TRUE)
end_time = Sys.time()
end_time - start_time
print("time for empir prob and voxel IDs")
save(list = ls(all.names = TRUE),
file=paste0(tempdir, "/step1.RData"))
}
mapping = read.table(paste0(PATH_TEMP, "/voxel_IDs.dat"), header=F)
mapping = as.matrix(mapping, nrow = 1)
n_subsets = ceiling(length(mapping)/1000)
print("number of subsets of voxels")
print(n_subsets)
filename_subset = paste0(PATH_TEMP, "/GLM_subset_",1, "_", method_name, "_Nvars_", n_covs,"_results.RData")
load(filename_subset)
n_coefs = length(output[[1]]$parameter) #obtain number of coefficients since it may differ from n_covs
Sys.time()
mapping_fn_vol2(brain_mask = brain_mask,
tempdir = PATH_TEMP,
mapping = mapping,
n_subsets = n_subsets,
n_covs = n_covs,
n_coefs = n_coefs,
GLMmethod = method,
outputdir = PATH_RESULTS)
Sys.time()
PATH_PROJ = "D:/Neuroimaging/Simulations/LesionMaskSimulation/" # Project path
#
PATH_DATA = file.path(PATH_PROJ, 'data', 'simulated_data') # Path where simulated lesion masks are stored
training_dataset = read.table(paste0(PATH_DATA, "/GLM_sample1000.dat"), header=T)
#
PATH_TEMP = file.path(PATH_PROJ, 'data', 'temp') # Path where temporary files will be stored
PATH_RESULTS = file.path(PATH_PROJ, 'results', 'MeanBR') # Path where GLM results are saved
#PATH_RESULTS = file.path(PATH_PROJ, 'results', 'ML') # change to ML if you are planning to obtain MLEs
#
PATH_SRC = file.path(PATH_PROJ, 'src') # Path where some help functions are stored
source(paste0(PATH_SRC, "/empir_prob_step1.R"))
source(paste0(PATH_SRC, "/lesion_matrix_step2.R"))
source(paste0(PATH_SRC, "/glm_step3.R"))
source(paste0(PATH_SRC, "/map_to_masks_step4.R"))
mapping = read.table(paste0(PATH_TEMP, "/voxel_IDs.dat"), header=F)
mapping = as.matrix(mapping, nrow = 1)
n_subsets = ceiling(length(mapping)/1000)
print("number of subsets of voxels")
print(n_subsets)
filename_subset = paste0(PATH_TEMP, "/GLM_subset_",1, "_", method_name, "_Nvars_", n_covs,"_results.RData")
load(filename_subset)
n_coefs = length(output[[1]]$parameter) #obtain number of coefficients since it may differ from n_covs
Sys.time()
mapping_fn_vol2(brain_mask = brain_mask,
tempdir = PATH_TEMP,
mapping = mapping,
n_subsets = n_subsets,
n_covs = n_covs,
n_coefs = n_coefs,
GLMmethod = method,
outputdir = PATH_RESULTS)
Sys.time()
load(paste0(PATH_TEMP, "/step2.RData"))
#our proposed option is to parallelize in subsets of voxels
subset_size = 1000
n_subsets = ceiling(nrow(lesions_subj)/subset_size)
for(i in 1:n_subsets){
load(paste0(PATH_TEMP, "/step2.RData"))
#determine indices
if(i == n_subsets) {
if(subset_size*(i-1)+1==nrow(lesions_subj)) {
subset_idx = nrow(lesions_subj)
} else {
subset_idx = seq(subset_size*(i-1)+1, nrow(lesions_subj), by = 1)
}
} else {
subset_idx = seq(subset_size*(i - 1)+1, subset_size*i , by = 1)
}
print(i)
lesions_subj_temp= lesions_subj[subset_idx,]
print(dim(lesions_subj_temp))
if(length(subset_idx)==1) {
lesions_subj_temp = as.matrix(as.vector(lesions_subj[subset_idx,]), nrow=1)
lesions_subj_temp = t(lesions_subj_temp)
print(dim(lesions_subj_temp))
}
rm(lesions_subj)
gc()
n_cores = 2
#run GLM
glm_results = fit_glm_fn(datafile = training_dataset,
lesionmat = lesions_subj_temp,
n_covs_cat = n_covs_cat,
n_covs_cont = n_covs_cont,
GLMmethod = method,
link_fn = link_fn,
outputdir = PATH_TEMP,
subset = i, n_cores = n_cores)
rm(glm_results)
gc()
print(i)
}

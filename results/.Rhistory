#install.packages("rmarkdown")
knitr::opts_chunk$set(echo = TRUE)
#install.packages("oro.nifti")
#install.packages("pryr")
#install.packages("enrichwith")
#install.packages("lpSolveAPI")
#install.packages("brglm2")
#install.packages("detectseparation")
#install.packages("devtools")
#devtools::install_github("ikosmidis/waldi", force=T)
#install.packages("foreach")
#install.packages("doParallel")
#install.packages("iterators")
library(oro.nifti)
library(pryr)
library(enrichwith)
library(lpSolveAPI)
library(brglm2)
library(detectseparation)
library(devtools)
library(waldi)
library(foreach)
library(doParallel)
library(iterators)
n_covs_cat = 0 # number of categorical covariates
n_covs_cont = 1 # number of continuous covariates
n_covs = n_covs_cat + n_covs_cont
empir_avail = 1 # flag: 0 (empirical probability is not available), 1 (empirical probability is available)
n_cores = 2 # number of cores for parallel GLM
link_fn = "probit" # options are logit and probit
method = 2 # 1: ML, 2: MeanBR
method_name = "MeanBR" # "ML" or "MeanBR"
brain_mask = readNIfTI(paste0(PATH_PROJ, "/data/MNI152_T1_2mm_brain_mask.nii.gz")) #MNI 2mm brain mask
n_covs_cat = 0 # number of categorical covariates
n_covs_cont = 1 # number of continuous covariates
n_covs = n_covs_cat + n_covs_cont
empir_avail = 1 # flag: 0 (empirical probability is not available), 1 (empirical probability is available)
n_cores = 2 # number of cores for parallel GLM
link_fn = "probit" # options are logit and probit
method = 2 # 1: ML, 2: MeanBR
method_name = "MeanBR" # "ML" or "MeanBR"
brain_mask = readNIfTI(paste0(PATH_PROJ, "/data/MNI152_T1_2mm_brain_mask.nii")) #MNI 2mm brain mask
PATH_PROJ = "D:/Neuroimaging/Simulations/LesionMaskSimulation/" # Project path
#
PATH_DATA = file.path(PATH_PROJ, 'data', 'simulated_data') # Path where simulated lesion masks are stored
training_dataset = read.table(paste0(PATH_DATA, "/GLM_sample1000.dat"), header=T)
#
PATH_TEMP = file.path(PATH_PROJ, 'data', 'temp') # Path where temporary files will be stored
PATH_RESULTS = file.path(PATH_PROJ, 'results', 'MeanBR') # Path where GLM results are saved
#PATH_RESULTS = file.path(PATH_PROJ, 'results', 'ML') # change to ML if you are planning to obtain MLEs
#
PATH_SRC = file.path(PATH_PROJ, 'src') # Path where some help functions are stored
source(paste0(PATH_SRC, "/empir_prob_step1.R"))
source(paste0(PATH_SRC, "/lesion_matrix_step2.R"))
source(paste0(PATH_SRC, "/glm_step3.R"))
source(paste0(PATH_SRC, "/map_to_masks_step4.R"))
n_covs_cat = 0 # number of categorical covariates
n_covs_cont = 1 # number of continuous covariates
n_covs = n_covs_cat + n_covs_cont
empir_avail = 1 # flag: 0 (empirical probability is not available), 1 (empirical probability is available)
n_cores = 2 # number of cores for parallel GLM
link_fn = "probit" # options are logit and probit
method = 2 # 1: ML, 2: MeanBR
method_name = "MeanBR" # "ML" or "MeanBR"
brain_mask = readNIfTI(paste0(PATH_PROJ, "/data/MNI152_T1_2mm_brain_mask.nii")) #MNI 2mm brain mask
if(!empir_avail) {
start_time = Sys.time()
temp = empir_prob(datafile = training_dataset,
imagedir = PATH_DATA,
outputdir = PATH_TEMP,
voxel_IDs = TRUE)
end_time = Sys.time()
end_time - start_time
print("time for empir prob and voxel IDs")
#save(list = ls(all.names = TRUE),
#      file=paste0(tempdir, "/step1.RData"))
}
if(empir_avail) {
empir_prob_mask = readNIfTI(paste0(PATH_DATA, "/empir_prob_mask.nii")) #read in empirical probability mask
voxel_idx = as.matrix(which(empir_prob_mask!=0), col = 1)
# add the first null voxel, i.e. lesion-free
#voxel_idx = rbind(which(empir_prob_mask == 0)[1], voxel_idx)
print(dim(voxel_idx))
write.table(voxel_idx, file=paste0(PATH_TEMP,"/voxel_IDs.dat"), sep=" ", row.names = F, col.names = F)
voxel_IDs = read.table(paste0(PATH_TEMP, "/voxel_IDs.dat"), header=F)
} else {
voxel_IDs = read.table(paste0(PATH_TEMP, "/voxel_IDs.dat"), header=F)
}
voxel_IDs = as.matrix(voxel_IDs, nrow = 1)
start_time = Sys.time()
lesions_subj = all_subj(datafile = training_dataset,
imagedir = PATH_DATA,
voxel_IDs = voxel_IDs)
end_time = Sys.time()
end_time - start_time
save(lesions_subj,
file = paste0(PATH_TEMP, "/step2.RData"))
dim(lesions_subj)
#our proposed option is to parallelize in subsets of voxels
subset_size = 1000
n_subsets = ceiling(nrow(lesions_subj)/subset_size)
i = 1
load(paste0(PATH_TEMP, "/step2.RData"))
#determine indices
if(i == n_subsets) {
if(subset_size*(i-1)+1==nrow(lesions_subj)) {
subset_idx = nrow(lesions_subj)
} else {
subset_idx = seq(subset_size*(i-1)+1, nrow(lesions_subj), by = 1)
}
} else {
subset_idx = seq(subset_size*(i - 1)+1, subset_size*i , by = 1)
}
subset_idx
print(i)
lesions_subj_temp= lesions_subj[subset_idx,]
print(dim(lesions_subj_temp))
if(length(subset_idx)==1) {
lesions_subj_temp = as.matrix(as.vector(lesions_subj[subset_idx,]), nrow=1)
lesions_subj_temp = t(lesions_subj_temp)
print(dim(lesions_subj_temp))
}
rm(lesions_subj)
gc()
#n_cores = 2
#run GLM
glm_results = fit_glm_fn(datafile = training_dataset,
lesionmat = lesions_subj_temp,
n_covs_cat = n_covs_cat,
n_covs_cont = n_covs_cont,
GLMmethod = method,
link_fn = link_fn,
outputdir = PATH_TEMP,
subset = i, n_cores = n_cores)
z_age=rep(0,1000)
z_age=rep(0,1000)
for (i in 1:1000){
z_age[i] = glm_results[[i]]$zscore[2]
}
hist(z_age)
z_age=rep(0,1000)
for (i in 1:1000){
z_age[i] = glm_results[[i]]$estimate[2]
}
hist(z_age)
#install.packages("rmarkdown")
knitr::opts_chunk$set(echo = TRUE)
#install.packages("oro.nifti")
#install.packages("pryr")
#install.packages("enrichwith")
#install.packages("lpSolveAPI")
#install.packages("brglm2")
#install.packages("detectseparation")
#install.packages("devtools")
#devtools::install_github("ikosmidis/waldi", force=T)
#install.packages("foreach")
#install.packages("doParallel")
#install.packages("iterators")
library(oro.nifti)
library(pryr)
library(enrichwith)
library(lpSolveAPI)
library(brglm2)
library(detectseparation)
library(devtools)
library(waldi)
library(foreach)
library(doParallel)
library(iterators)
PATH_PROJ = "D:/Neuroimaging/Simulations/LesionMaskSimulation/" # Project path
#
PATH_DATA = file.path(PATH_PROJ, 'data', 'simulated_data') # Path where simulated lesion masks are stored
training_dataset = read.table(paste0(PATH_DATA, "/GLM_sample1000.dat"), header=T)
#
PATH_TEMP = file.path(PATH_PROJ, 'data', 'temp') # Path where temporary files will be stored
PATH_RESULTS = file.path(PATH_PROJ, 'results', 'MeanBR') # Path where GLM results are saved
#PATH_RESULTS = file.path(PATH_PROJ, 'results', 'ML') # change to ML if you are planning to obtain MLEs
#
PATH_SRC = file.path(PATH_PROJ, 'src') # Path where some help functions are stored
source(paste0(PATH_SRC, "/empir_prob_step1.R"))
source(paste0(PATH_SRC, "/lesion_matrix_step2.R"))
source(paste0(PATH_SRC, "/glm_step3.R"))
source(paste0(PATH_SRC, "/map_to_masks_step4.R"))
n_covs_cat = 0 # number of categorical covariates
n_covs_cont = 1 # number of continuous covariates
n_covs = n_covs_cat + n_covs_cont
empir_avail = 1 # flag: 0 (empirical probability is not available), 1 (empirical probability is available)
n_cores = 2 # number of cores for parallel GLM
link_fn = "probit" # options are logit and probit
method = 2 # 1: ML, 2: MeanBR
method_name = "MeanBR" # "ML" or "MeanBR"
brain_mask = readNIfTI(paste0(PATH_PROJ, "/data/MNI152_T1_2mm_brain_mask.nii")) #MNI 2mm brain mask
load(paste0(PATH_TEMP, "/step2.RData"))
#our proposed option is to parallelize in subsets of voxels
subset_size = 1000
n_subsets = ceiling(nrow(lesions_subj)/subset_size)
for(i in 1:n_subsets){
load(paste0(PATH_TEMP, "/step2.RData"))
#determine indices
if(i == n_subsets) {
if(subset_size*(i-1)+1==nrow(lesions_subj)) {
subset_idx = nrow(lesions_subj)
} else {
subset_idx = seq(subset_size*(i-1)+1, nrow(lesions_subj), by = 1)
}
} else {
subset_idx = seq(subset_size*(i - 1)+1, subset_size*i , by = 1)
}
print(i)
lesions_subj_temp= lesions_subj[subset_idx,]
print(dim(lesions_subj_temp))
if(length(subset_idx)==1) {
lesions_subj_temp = as.matrix(as.vector(lesions_subj[subset_idx,]), nrow=1)
lesions_subj_temp = t(lesions_subj_temp)
print(dim(lesions_subj_temp))
}
rm(lesions_subj)
gc()
#n_cores = 2
#run GLM
glm_results = fit_glm_fn(datafile = training_dataset,
lesionmat = lesions_subj_temp,
n_covs_cat = n_covs_cat,
n_covs_cont = n_covs_cont,
GLMmethod = method,
link_fn = link_fn,
outputdir = PATH_TEMP,
subset = i, n_cores = n_cores)
rm(glm_results)
gc()
print(i)
}
mapping = read.table(paste0(PATH_TEMP, "/voxel_IDs.dat"), header=F)
mapping = as.matrix(mapping, nrow = 1)
n_subsets = ceiling(length(mapping)/1000)
print("number of subsets of voxels")
print(n_subsets)
filename_subset = paste0(PATH_TEMP, "/GLM_subset_",1, "_", method_name, "_Nvars_", n_covs,"_results.RData")
load(filename_subset)
n_coefs = length(output[[1]]$parameter) #obtain number of coefficients since it may differ from n_covs
Sys.time()
mapping_fn_vol2(brain_mask = brain_mask,
tempdir = PATH_TEMP,
mapping = mapping,
n_subsets = n_subsets,
n_covs = n_covs,
n_coefs = n_coefs,
GLMmethod = method,
outputdir = PATH_RESULTS)
Sys.time()
age_coef = readNIfTI("D:/Neuroimaging/Simulations/LesionMaskSimulation/results/MeanBR/coef_age_nvars_1_method_2.nii.gz")
hist(age_coef[empir_prob_mask!=0])
summary(age_coef)
summary(age_coef[empir_prob_mask!=0])
length(mapping)
mapping[200]
mapping[200,1]
n_subsets
n_covs
method
PATH_RESULTS
load("D:/Neuroimaging/Simulations/LesionMaskSimulation/data/temp/GLM_subset_12_meanBR_Nvars_1_results.RData")
View(mapping)
source(paste0(PATH_SRC, "/map_to_masks_step4.R"))
mapping = read.table(paste0(PATH_TEMP, "/voxel_IDs.dat"), header=F)
mapping = as.matrix(mapping, nrow = 1)
n_subsets = ceiling(length(mapping)/1000)
print("number of subsets of voxels")
print(n_subsets)
filename_subset = paste0(PATH_TEMP, "/GLM_subset_",1, "_", method_name, "_Nvars_", n_covs,"_results.RData")
n_coefs = length(output[[1]]$parameter) #obtain number of coefficients since it may differ from n_covs
Sys.time()
mapping_fn_vol2(brain_mask = brain_mask,
tempdir = PATH_TEMP,
mapping = mapping,
n_subsets = n_subsets,
n_covs = n_covs,
n_coefs = n_coefs,
GLMmethod = method,
outputdir = PATH_RESULTS)
coefs_ml = readNIfTI("D:/Neuroimaging/Simulations/LesionMaskSimulation/results/MeanBR/coef_(Intercept)_nvars_1_method_2.nii.gz")
summary(coefs_ml[empir_prob_mask!=0])
source(paste0(PATH_SRC, "/map_to_masks_step4.R"))
mapping = read.table(paste0(PATH_TEMP, "/voxel_IDs.dat"), header=F)
mapping = as.matrix(mapping, nrow = 1)
n_subsets = ceiling(length(mapping)/1000)
print("number of subsets of voxels")
print(n_subsets)
filename_subset = paste0(PATH_TEMP, "/GLM_subset_",1, "_", method_name, "_Nvars_", n_covs,"_results.RData")
load(filename_subset)
n_coefs = length(output[[1]]$parameter) #obtain number of coefficients since it may differ from n_covs
Sys.time()
mapping_fn_vol2(brain_mask = brain_mask,
tempdir = PATH_TEMP,
mapping = mapping,
n_subsets = n_subsets,
n_covs = n_covs,
n_coefs = n_coefs,
GLMmethod = method,
outputdir = PATH_RESULTS)
source(paste0(PATH_SRC, "/map_to_masks_step4.R"))
mapping = read.table(paste0(PATH_TEMP, "/voxel_IDs.dat"), header=F)
mapping = as.matrix(mapping, nrow = 1)
n_subsets = ceiling(length(mapping)/1000)
print("number of subsets of voxels")
print(n_subsets)
filename_subset = paste0(PATH_TEMP, "/GLM_subset_",1, "_", method_name, "_Nvars_", n_covs,"_results.RData")
load(filename_subset)
n_coefs = length(output[[1]]$parameter) #obtain number of coefficients since it may differ from n_covs
Sys.time()
mapping_fn_vol2(brain_mask = brain_mask,
tempdir = PATH_TEMP,
mapping = mapping,
n_subsets = n_subsets,
n_covs = n_covs,
n_coefs = n_coefs,
GLMmethod = method,
outputdir = PATH_RESULTS)
Sys.time()
coefs_br = readNIfTI("D:/Neuroimaging/Simulations/LesionMaskSimulation/results/MeanBR/coef_age_nvars_1_method_2.nii.gz")
summary(coef_br[empir_prob_mask!=0])
coefs_br = readNIfTI("D:/Neuroimaging/Simulations/LesionMaskSimulation/results/MeanBR/coef_age_nvars_1_method_2.nii.gz")
summary(coefs_br[empir_prob_mask!=0])
summary(coefs_br[empir_prob_mask!=0&brain_mask==1])
empir_prob_mask[mapping]
